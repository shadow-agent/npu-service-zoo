{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations Loaded Successfully:\n",
      "{'device': {'type': 'GPU', 'model': 'A5000', 'count': 1}, 'model': {'name': 'llama3.3:70b', 'quantization': 'Q4_K_M', 'calibration': 'base'}, 'evaluation': {'task': 'chatbot', 'metrics': {'precision': True, 'recall': True, 'f1': True, 'tps': False, 'power_consumption': False, 'memory_usage': False}, 'output_dir': './results/chatbot'}}\n",
      "Result file does not exist. Proceeding with computation...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Configuration 파일 경로\n",
    "config_path = \"./configuration/config.yaml\"\n",
    "\n",
    "def load_configurations(config_path):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        configs = yaml.safe_load(file)\n",
    "    return configs\n",
    "\n",
    "# Configuration 로드\n",
    "configs = load_configurations(config_path)\n",
    "print(\"Configurations Loaded Successfully:\")\n",
    "print(configs)\n",
    "\n",
    "# 주요 변수 추출\n",
    "device_config = configs[\"device\"]\n",
    "model_config = configs[\"model\"]\n",
    "evaluation_settings = configs[\"evaluation\"]\n",
    "active_metrics = evaluation_settings[\"metrics\"]\n",
    "output_dir = evaluation_settings[\"output_dir\"]\n",
    "\n",
    "# 결과 파일 이름 생성\n",
    "name_config = (\n",
    "    f'{device_config[\"type\"]}-'\n",
    "    f'{device_config[\"model\"]}_'\n",
    "    f'{model_config[\"name\"]}-'\n",
    "    f'{model_config[\"quantization\"]}_'\n",
    "    f'calib-{model_config.get(\"calibration\", \"none\")}'\n",
    ")\n",
    "result_folder = os.path.join(output_dir, 'coqa')\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.txt\")\n",
    "\n",
    "# Ensure result folder exists\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    print(f\"Result file already exists at: {result_file}. Skipping computation.\")\n",
    "    proceed_calc = False\n",
    "else:\n",
    "    print(f\"Result file does not exist. Proceeding with computation...\")\n",
    "    proceed_calc = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ./data/chatbot/coqa/dev.json\n",
      "Number of samples in dataset: 7983\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from src.chatbot.dataset import load_data, get_inputs\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Prompt 리스트 생성\n",
    "numTest = 300\n",
    "\n",
    "def get_prompt_list(data_folder, eval_dataset, model_name, num_ctx, max_output_len):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('nvidia/ChatQA-1.5-8B')\n",
    "    dataset_paths = {\"coqa\": \"dev.json\"}\n",
    "\n",
    "    if eval_dataset in dataset_paths:\n",
    "        input_path = os.path.join(data_folder, dataset_paths[eval_dataset])\n",
    "    else:\n",
    "        raise Exception(\"Invalid dataset name provided.\")\n",
    "\n",
    "    data_list = load_data(input_path)\n",
    "    print(f\"Number of samples in dataset: {len(data_list)}\")\n",
    "    return get_inputs(data_list, eval_dataset, tokenizer, num_ctx=num_ctx, max_output_len=max_output_len)\n",
    "\n",
    "# LLM 응답 생성\n",
    "def generate_responses(prompt_list, llm_model, max_tokens, bos_token=\"<|begin_of_text|>\"):\n",
    "    llm = Ollama(\n",
    "        model=llm_model,\n",
    "        temperature=0,\n",
    "        request_timeout=600,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    output_list = []\n",
    "    for prompt in prompt_list[:numTest]:\n",
    "        prompt = bos_token + prompt\n",
    "        response = llm.complete(prompt).text.strip().replace(\"\\n\", \" \")\n",
    "        output_list.append(response)\n",
    "    return output_list\n",
    "\n",
    "if proceed_calc:\n",
    "    prompt_list = get_prompt_list(\n",
    "        data_folder='./data/chatbot/coqa/',\n",
    "        eval_dataset='coqa',\n",
    "        model_name=model_config[\"name\"],\n",
    "        num_ctx=3,\n",
    "        max_output_len=128\n",
    "    )\n",
    "    output_list = generate_responses(\n",
    "        prompt_list, llm_model=model_config[\"name\"], max_tokens=200\n",
    "    )\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(result_file, \"w\") as f:\n",
    "        for output in output_list:\n",
    "            f.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 7983\n",
      "Method: default; Precision: 0.5517; recall: 0.7088; f1: 0.6773\n",
      "Processed and added GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.txt to leaderboard.\n",
      "Leaderboard updated and saved to ./results/chatbot/coqa/leaderboard.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3029065/2277672481.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chatbot.get_scores import evaluate_f1\n",
    "\n",
    "# 리더보드 업데이트 함수\n",
    "def update_leaderboard(result_dir, leaderboard_file, active_metrics, ground_truth_file):\n",
    "    # 기존 리더보드 파일 로드 또는 새로 생성\n",
    "    if os.path.exists(leaderboard_file):\n",
    "        leaderboard = pd.read_csv(leaderboard_file)\n",
    "    else:\n",
    "        leaderboard = pd.DataFrame(columns=[\n",
    "            \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "            *active_metrics  # 활성화된 메트릭만 포함\n",
    "        ])\n",
    "\n",
    "    # 결과 디렉토리 파일 처리\n",
    "    for filename in os.listdir(result_dir):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        # 파일 이름에서 메타데이터 추출\n",
    "        name_head = filename.replace(\".txt\", \"\")\n",
    "        metadata = {\n",
    "            \"device-type\": name_head.split(\"-\")[0],\n",
    "            \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "            \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "            \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "            \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "        }\n",
    "\n",
    "        # 리더보드에 이미 존재하는 데이터인지 확인\n",
    "        if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "            (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "            (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "            (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "            (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "            print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "            continue\n",
    "\n",
    "        # 결과 파일 로드\n",
    "        result_file = os.path.join(result_dir, filename)\n",
    "\n",
    "        # F1, Precision, Recall 계산\n",
    "        precision, recall, f1 = evaluate_f1(ground_truth_file, result_file)\n",
    "\n",
    "        # 평균 메트릭 계산\n",
    "        avg_metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "\n",
    "        # 새 항목 생성 및 리더보드 업데이트\n",
    "        new_entry = pd.DataFrame([{\n",
    "            **metadata,\n",
    "            **avg_metrics\n",
    "        }])\n",
    "        leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "        print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "    # 리더보드 저장\n",
    "    leaderboard.to_csv(leaderboard_file, index=False)\n",
    "    print(f\"Leaderboard updated and saved to {leaderboard_file}\")\n",
    "\n",
    "# 리더보드 업데이트 실행\n",
    "result_dir = \"./results/chatbot/coqa\"\n",
    "leaderboard_file = os.path.join(result_dir, \"leaderboard.csv\")\n",
    "ground_truth_file = \"./data/chatbot/coqa/dev.json\"\n",
    "active_metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "update_leaderboard(result_dir, leaderboard_file, active_metrics, ground_truth_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
