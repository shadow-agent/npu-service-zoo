{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations Loaded Successfully:\n",
      "{'device': {'type': 'NPU', 'model': 'RNGD', 'count': 1}, 'model': {'name': 'llama3.1-8B-Instruct', 'quantization': 'W8A8', 'calibration': 'base'}, 'evaluation': {'task': 'chatbot', 'metrics': {'precision': True, 'recall': True, 'f1': True, 'tps': True, 'power_consumption': False, 'memory_usage': False}, 'output_dir': './results/chatbot'}}\n",
      "Result file does not exist. Proceeding with computation...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from time import time\n",
    "\n",
    "# Configuration 파일 경로\n",
    "config_path = \"./configuration/config.yaml\"\n",
    "\n",
    "def load_configurations(config_path):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        configs = yaml.safe_load(file)\n",
    "    return configs\n",
    "\n",
    "# Configuration 로드\n",
    "configs = load_configurations(config_path)\n",
    "print(\"Configurations Loaded Successfully:\")\n",
    "print(configs)\n",
    "\n",
    "# 주요 변수 추출\n",
    "device_config = configs[\"device\"]\n",
    "model_config = configs[\"model\"]\n",
    "evaluation_settings = configs[\"evaluation\"]\n",
    "active_metrics = evaluation_settings[\"metrics\"]\n",
    "output_dir = evaluation_settings[\"output_dir\"]\n",
    "\n",
    "# 결과 파일 이름 생성\n",
    "name_config = (\n",
    "    f'{device_config[\"type\"]}-'\n",
    "    f'{device_config[\"model\"]}_'\n",
    "    f'{model_config[\"name\"]}-'\n",
    "    f'{model_config[\"quantization\"]}_'\n",
    "    f'calib-{model_config.get(\"calibration\", \"none\")}'\n",
    ")\n",
    "result_folder = os.path.join(output_dir, 'coqa')\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.txt\")\n",
    "\n",
    "# Ensure result folder exists\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    print(f\"Result file already exists at: {result_file}. Skipping computation.\")\n",
    "    proceed_calc = False\n",
    "else:\n",
    "    print(f\"Result file does not exist. Proceeding with computation...\")\n",
    "    proceed_calc = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-01-10 10:19:50 WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.1.0+cu121)\n",
      "    Python  3.10.15 (you have 3.10.15)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "INFO:2025-01-10 10:19:52 Prefill buckets: [Bucket(batch_size=1, attention_size=512), Bucket(batch_size=1, attention_size=1024)]\n",
      "INFO:2025-01-10 10:19:52 Decode buckets: [Bucket(batch_size=64, attention_size=2048), Bucket(batch_size=128, attention_size=2048)]\n",
      "INFO:2025-01-10 10:19:52 For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "/home/elicer/anaconda3/envs/jun-rngd/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from furiosa_llm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"RUST_BACKTRACE\"] = \"full\"\n",
    "furiosa_llm = LLM.from_artifacts(\"/home/elicer/renegade/Llama-3.1-8B-Instruct\", devices=\"npu:1:*\")\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from ./data/chatbot/coqa/dev.json\n",
      "Number of samples in dataset: 7983\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from src.chatbot.dataset import load_data, get_inputs, get_inputs_npu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Prompt 리스트 생성\n",
    "numTest = 7983\n",
    "\n",
    "def get_prompt_list(data_folder, eval_dataset, model_name, num_ctx, max_output_len):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('nvidia/ChatQA-1.5-8B')\n",
    "    dataset_paths = {\"coqa\": \"dev.json\"}\n",
    "\n",
    "    if eval_dataset in dataset_paths:\n",
    "        input_path = os.path.join(data_folder, dataset_paths[eval_dataset])\n",
    "    else:\n",
    "        raise Exception(\"Invalid dataset name provided.\")\n",
    "\n",
    "    data_list = load_data(input_path)\n",
    "    print(f\"Number of samples in dataset: {len(data_list)}\")\n",
    "    return get_inputs_npu(data_list, eval_dataset, tokenizer, num_ctx=num_ctx, max_output_len=max_output_len)\n",
    "\n",
    "prompt_list = get_prompt_list(\n",
    "    data_folder='./data/chatbot/coqa/',\n",
    "    eval_dataset='coqa',\n",
    "    model_name=model_config[\"name\"],\n",
    "    num_ctx=3,\n",
    "    max_output_len=200\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SamplingParams(n=1, best_of=1, temperature=0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, max_tokens=400, min_tokens=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npu_generate_responses(prompt_list, sampling_params, bos_token=\"<|begin_of_text|>\", num_test=5, max_length=1024):\n",
    "    \"\"\"\n",
    "    NPU 기반의 응답 생성 함수 - 입력 길이 제한 적용\n",
    "\n",
    "    Args:\n",
    "    - prompt_list: 프롬프트 리스트\n",
    "    - sampling_params: 샘플링 파라미터\n",
    "    - bos_token: 프롬프트 시작 토큰\n",
    "    - num_test: 테스트할 프롬프트 수\n",
    "    - max_length: NPU 모델의 최대 입력 길이\n",
    "\n",
    "    Returns:\n",
    "    - output_list: 생성된 응답 리스트\n",
    "    - tps: 초당 토큰 수 리스트\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "    tps = []\n",
    "\n",
    "    for prompt in prompt_list[:num_test]:\n",
    "\n",
    "        # NPU에서 응답 생성 시작\n",
    "        start_time = time()\n",
    "        try:\n",
    "            output = furiosa_llm.generate(prompt, sampling_params)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            output_list.append(\"<ERROR>\")\n",
    "            tps.append(0)\n",
    "            continue\n",
    "\n",
    "        elapsed_time = time() - start_time\n",
    "\n",
    "        # 응답 처리\n",
    "        generated_text = output.outputs[0].text[2:].strip().replace(\"\\n\", \" \")\n",
    "\n",
    "        # 초당 토큰 수 계산\n",
    "        len_tokens = len(word_tokenize(prompt))\n",
    "        tps.append(len_tokens / elapsed_time if elapsed_time > 0 else 0)\n",
    "\n",
    "        # 결과 저장\n",
    "        output_list.append(generated_text)\n",
    "\n",
    "    return output_list, tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1116\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1131\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1143\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1152\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1147\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1154\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1153\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1158\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1158\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1156\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1156\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1159\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1153\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1150\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1153\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1154\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1158\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1152\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1152\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1157\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1155\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1030\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1044\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1056\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1055\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1052\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1042\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1041\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1039\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1048\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1041\n",
      "Error generating response: unsupported input: input sequence too long - max: 1024, received: 1044\n"
     ]
    }
   ],
   "source": [
    "output_list, tps_list = npu_generate_responses(\n",
    "        prompt_list, sampling_params, num_test=len(prompt_list)\n",
    "    )\n",
    "# 결과 저장\n",
    "with open(result_file, \"w\") as f:\n",
    "    for output in output_list:\n",
    "        f.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping GPU-A100_llama3.1-Q4_K_M_calib-base.txt, already in leaderboard.\n",
      "Skipping GPU-A100_llama3.1:70b-Q4_K_M_calib-base.txt, already in leaderboard.\n",
      "Skipping GPU-A100_qwen2.5:72b-Q4_K_M_calib-base.txt, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.1-Q4_K_M_calib-base.txt, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.1:70b-Q4_K_M_calib-base.txt, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.txt, already in leaderboard.\n",
      "7983 7983\n",
      "Method: default; Precision: 0.6995; recall: 0.7344; f1: 0.7929\n",
      "Processed and added NPU-RNGD_llama3.1-8B-Instruct-W8A8_calib-base.txt to leaderboard.\n",
      "Leaderboard updated and saved to ./results/chatbot/coqa/leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.chatbot.get_scores import evaluate_f1\n",
    "\n",
    "# 리더보드 업데이트 함수\n",
    "def update_leaderboard(result_dir, leaderboard_file, active_metrics, ground_truth_file):\n",
    "    # 기존 리더보드 파일 로드 또는 새로 생성\n",
    "    if os.path.exists(leaderboard_file):\n",
    "        leaderboard = pd.read_csv(leaderboard_file)\n",
    "    else:\n",
    "        leaderboard = pd.DataFrame(columns=[\n",
    "            \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "            *active_metrics  # 활성화된 메트릭만 포함\n",
    "        ])\n",
    "\n",
    "    # 결과 디렉토리 파일 처리\n",
    "    for filename in os.listdir(result_dir):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        # 파일 이름에서 메타데이터 추출\n",
    "        name_head = filename.replace(\".txt\", \"\")\n",
    "        metadata = {\n",
    "            \"device-type\": name_head.split(\"-\")[0],\n",
    "            \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "            \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "            \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "            \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "        }\n",
    "\n",
    "        # 리더보드에 이미 존재하는 데이터인지 확인\n",
    "        if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "            (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "            (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "            (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "            (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "            print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "            continue\n",
    "\n",
    "        # 결과 파일 로드\n",
    "        result_file = os.path.join(result_dir, filename)\n",
    "\n",
    "        # F1, Precision, Recall 계산\n",
    "        precision, recall, f1 = evaluate_f1(ground_truth_file, result_file)\n",
    "        median_tps = np.median(tps_list) if tps_list else np.nan\n",
    "        \n",
    "        metrics = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"tps\": median_tps\n",
    "        }\n",
    "\n",
    "        # 새 항목 생성 및 리더보드 업데이트\n",
    "        new_entry = pd.DataFrame([{\n",
    "            **metadata,\n",
    "            **metrics\n",
    "        }])\n",
    "        leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "        print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "    # 리더보드 저장\n",
    "    leaderboard.to_csv(leaderboard_file, index=False)\n",
    "    print(f\"Leaderboard updated and saved to {leaderboard_file}\")\n",
    "\n",
    "# 리더보드 업데이트 실행\n",
    "result_dir = \"./results/chatbot/coqa\"\n",
    "leaderboard_file = os.path.join(result_dir, \"leaderboard.csv\")\n",
    "ground_truth_file = \"./data/chatbot/coqa/dev.json\"\n",
    "\n",
    "update_leaderboard(result_dir, leaderboard_file, active_metrics, ground_truth_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun-rngd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
