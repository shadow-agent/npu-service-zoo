{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서비스 지표 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경 탐지 (GPU, NPU-FuriosaAI RNGD)\n",
    "\n",
    "- 서비스 실행 (예: 번역 + 모니터링)\n",
    "\n",
    "- 스코어 계산하기 \n",
    "\n",
    "- 리더보드에 자동 업데이트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./configuration/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations Loaded Successfully:\n",
      "Active Metrics: ['BLEU', 'METEOR', 'BERTScore', 'tps']\n",
      "Device Config: {'type': 'GPU', 'model': 'A5000', 'count': 1}\n",
      "Model Config: {'name': 'mixtral:latest', 'quantization': 'Q4_K_M', 'calibration': 'base'}\n",
      "Evaluation Settings: {'task': 'translation', 'output_dir': './results/translation'}\n",
      "----------------------- \n",
      "\n",
      "Result file does not exist. Proceeding with computation...\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from src.common.load_config import load_all_configurations\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load configuration\n",
    "all_configs = load_all_configurations(config_path)\n",
    "\n",
    "# Set ENABLE_MONITORING=true in the environment to enable monitoring\n",
    "os.environ[\"ENABLE_MONITORING\"] = \"true\" if \"power_consumption\" in all_configs[\"active_metrics\"] else \"false\"\n",
    "    \n",
    "# Print the loaded configurations\n",
    "print(\"Configurations Loaded Successfully:\")\n",
    "print(\"Active Metrics:\", all_configs[\"active_metrics\"])\n",
    "print(\"Device Config:\", all_configs[\"device_config\"])\n",
    "print(\"Model Config:\", all_configs[\"model_config\"])\n",
    "print(\"Evaluation Settings:\", all_configs[\"evaluation_settings\"])\n",
    "\n",
    "# Look up imtermediate result folder to avoid redundancy (performance leaderboard)\n",
    "name_config = (\n",
    "    f'{all_configs[\"device_config\"][\"type\"]}-'\n",
    "    f'{all_configs[\"device_config\"][\"model\"]}_'\n",
    "    f'{all_configs[\"model_config\"][\"name\"]}-'\n",
    "    f'{all_configs[\"model_config\"][\"quantization\"]}_'\n",
    "    f'calib-{all_configs[\"model_config\"].get(\"calibration\", \"none\")}'\n",
    ")\n",
    "result_folder = os.path.join(all_configs[\"evaluation_settings\"][\"output_dir\"], \"Korean2English\")\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.json\")\n",
    "\n",
    "# Ensure result folder exists\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "print(\"----------------------- \\n\")\n",
    "if os.path.exists(result_file):\n",
    "    print(f\"Result file already exists at: {result_file}. Skipping computation.\")\n",
    "    proceed_calc = \"true\"\n",
    "else:\n",
    "    print(f\"Result file does not exist. Proceeding with computation...\")\n",
    "    proceed_calc = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task-specific modules based on the configuration\n",
    "task = all_configs[\"evaluation_settings\"].get(\"task\")\n",
    "\n",
    "if task == \"translation\":\n",
    "    from src.translation.one_translate import (\n",
    "        initialize_translation_environment,\n",
    "        translate_text,\n",
    "        batch_translate_text,\n",
    "    )\n",
    "    from src.metrics.evaluate_translation import evaluate_translation\n",
    "elif task == \"summarization\":\n",
    "    from src.summarization.one_summary import (\n",
    "        initialize_summarization_environment,\n",
    "        summarize_text,\n",
    "        batch_summarize_text,\n",
    "    )\n",
    "    from src.metrics.evaluate_summarization import evaluate_summarization\n",
    "elif task == \"multimodal\":\n",
    "    from src.multimodal.one_multimodal import (\n",
    "        initialize_multimodal_environment,\n",
    "        process_multimodal_data,\n",
    "    )\n",
    "    from src.metrics.evaluate_multimodal import evaluate_multimodal\n",
    "elif task == \"chatbot\":\n",
    "    from src.chatbot.one_chatbot import (\n",
    "        initialize_chatbot_environment,\n",
    "        generate_chat_response,\n",
    "    )\n",
    "    from src.metrics.evaluate_chatbot import evaluate_chatbot\n",
    "else:\n",
    "    raise ValueError(f\"Undetermined task: {task}. Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 벤치마킹 데이터셋 (FLORES-200)\n",
    "\n",
    "- 한 -> 영, 영 -> 한 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# FLORES-200 데이터 위치 \n",
    "data_dir = \"./data/translation/flores\"\n",
    "\n",
    "data_eng = load_text_file(f\"{data_dir}/devtest.eng_Latn\")\n",
    "data_kor = load_text_file(f\"{data_dir}/devtest.kor_Hang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 04:11:29,335 - ERROR - Failed to detect NPUs: [Errno 2] No such file or directory: 'furiosa-smi'\n",
      "2025-01-09 04:11:29,336 - INFO - Translation environment detected: GPU. Using GPU for translations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 04:11:31,825 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "ename": "HTTPStatusError",
     "evalue": "Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m num_benchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;66;03m# start time\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_translate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_kor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_benchmark\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKorean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m batch_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Add elapsed time to the results\u001b[39;00m\n\u001b[1;32m     13\u001b[0m result_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(all_configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKorean2English\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/home/dudaji/Jun/npu-service-zoo/src/translation/one_translate.py:82\u001b[0m, in \u001b[0;36mbatch_translate_text\u001b[0;34m(input_texts, source_language, target_language)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m input_texts:\n\u001b[1;32m     81\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 82\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_language\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     85\u001b[0m     translations\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m: result,\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed_time\n\u001b[1;32m     88\u001b[0m     })\n",
      "File \u001b[0;32m/data/home/dudaji/Jun/npu-service-zoo/src/translation/one_translate.py:64\u001b[0m, in \u001b[0;36mtranslate_text\u001b[0;34m(input_text, source_language, target_language)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _translation_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation environment not initialized. Call initialize_translation_environment first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_translation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_language\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_language\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/home/dudaji/Jun/npu-service-zoo/src/common/monitoring.py:22\u001b[0m, in \u001b[0;36mmonitor_power.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m _monitoring_enabled \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_MONITORING\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _monitoring_enabled:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Detect devices for monitoring\u001b[39;00m\n\u001b[1;32m     25\u001b[0m devices \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_devices\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m/data/home/dudaji/Jun/npu-service-zoo/src/translation/one_translate.py:31\u001b[0m, in \u001b[0;36minitialize_translation_environment.<locals>.<lambda>\u001b[0;34m(text, src, tgt)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _environment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m translate_with_gpu\n\u001b[0;32m---> 31\u001b[0m     _translation_function \u001b[38;5;241m=\u001b[39m monitor_power(\u001b[38;5;28;01mlambda\u001b[39;00m text, src, tgt: \u001b[43mtranslate_with_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation environment detected: GPU. Using GPU for translations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _environment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuriosa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/data/home/dudaji/Jun/npu-service-zoo/src/translation/gpu.py:61\u001b[0m, in \u001b[0;36mtranslate_with_gpu\u001b[0;34m(source_text, source_lang, target_lang, llm_model)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Format the prompt and generate the translation\u001b[39;00m\n\u001b[1;32m     56\u001b[0m full_prompt \u001b[38;5;241m=\u001b[39m translation_prompt\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     57\u001b[0m     source_lang\u001b[38;5;241m=\u001b[39msource_lang, \n\u001b[1;32m     58\u001b[0m     target_lang\u001b[38;5;241m=\u001b[39mtarget_lang, \n\u001b[1;32m     59\u001b[0m     source_text\u001b[38;5;241m=\u001b[39msource_text\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgpu_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:265\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    258\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    259\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    263\u001b[0m )\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:431\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    423\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    424\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m     },\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m    434\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    435\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m    436\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m    437\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/llama_index/llms/ollama/base.py:265\u001b[0m, in \u001b[0;36mOllama.complete\u001b[0;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mClient(timeout\u001b[38;5;241m=\u001b[39mTimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_timeout)) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m    261\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    262\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    263\u001b[0m         json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    264\u001b[0m     )\n\u001b[0;32m--> 265\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     raw \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    267\u001b[0m     text \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/langserve/lib/python3.11/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m error_type \u001b[38;5;241m=\u001b[39m error_types\u001b[38;5;241m.\u001b[39mget(status_class, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid status code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Server error '500 Internal Server Error' for url 'http://localhost:11434/api/generate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500"
     ]
    }
   ],
   "source": [
    "# Initialize the translation environment\n",
    "try:\n",
    "    initialize_translation_environment(llm_model=all_configs[\"model_config\"][\"name\"])\n",
    "except EnvironmentError as e:\n",
    "    print(f\"Failed to initialize translation environment: {e}\")\n",
    "    exit(1)\n",
    "    \n",
    "# 한 -> 영 번역 \n",
    "num_benchmark = 100\n",
    "start_time = time() # start time\n",
    "batch_results = batch_translate_text(data_kor[:num_benchmark], source_language=\"Korean\", target_language=\"English\")\n",
    "batch_results[\"elapsed_time\"] = time() - start_time  # Add elapsed time to the results\n",
    "result_folder = os.path.join(all_configs[\"evaluation_settings\"][\"output_dir\"], \"Korean2English\")\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.json\")\n",
    "print('ko2en translation done')\n",
    "\n",
    "# Save intermediate result \n",
    "with open(result_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(batch_results, file, indent=4, ensure_ascii=False)  # Save the batch results in JSON format\n",
    "    print(f\"Results saved to {result_file}.\")\n",
    "    \n",
    "# 영 -> 한 번역 \n",
    "start_time = time()\n",
    "batch_results = batch_translate_text(data_eng[:num_benchmark], source_language=\"English\", target_language=\"Korean\")\n",
    "batch_results[\"elapsed_time\"] = time() - start_time  # Add elapsed time to the results\n",
    "result_folder = os.path.join(all_configs[\"evaluation_settings\"][\"output_dir\"], \"English2Korean\")\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.json\")\n",
    "print('en2ko translation done')\n",
    "\n",
    "# Save intermediate result \n",
    "with open(result_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(batch_results, file, indent=4, ensure_ascii=False)  # Save the batch results in JSON format\n",
    "    print(f\"Results saved to {result_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and added GPU-A5000_qwen2:72b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Skipping GPU-A5000_llama3.1:70b-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.1-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Leaderboard updated and saved to ./results/translation/Korean2English/leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "from src.metrics.evaluate_translation import evaluate_translation\n",
    "\n",
    "# Configurations\n",
    "source_lang = \"Korean\"  # 소스 언어\n",
    "target_lang = \"English\"  # 목표 언어\n",
    "translation_config = f'{source_lang}2{target_lang}'\n",
    "\n",
    "result_dir = f\"./results/translation/{translation_config}/\"\n",
    "leaderboard_file = f\"./results/translation/{translation_config}/leaderboard.csv\"\n",
    "\n",
    "# 기존 리더보드 파일 로드 또는 새로 생성\n",
    "if os.path.exists(leaderboard_file):\n",
    "    leaderboard = pd.read_csv(leaderboard_file)\n",
    "else:\n",
    "    leaderboard = pd.DataFrame(columns=[\n",
    "        \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "        \"BLEU\", \"METEOR\", \"BERTScore\", \"tps\"\n",
    "    ])\n",
    "    \n",
    "# Process each JSON file\n",
    "for filename in os.listdir(result_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    name_head = filename.replace(\".json\", \"\")\n",
    "    metadata = {\n",
    "        \"device-type\": name_head.split(\"-\")[0],\n",
    "        \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "        \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "        \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "        \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "    }\n",
    "\n",
    "    # Skip if already in leaderboard\n",
    "    if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "        (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "        (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "        (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "        (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "        print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "        continue\n",
    "    \n",
    "    # Load translation results\n",
    "    with open(os.path.join(result_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "        translations = json_data.get(\"translations\", [])\n",
    "\n",
    "    # Evaluate translations\n",
    "    num_metrics = 4  # BLEU, METEOR, BERTScore, TPS\n",
    "    metrics = np.full((num_metrics, len(translations)), np.nan)\n",
    "\n",
    "    for i, result in enumerate(translations):\n",
    "        translation = result.get(\"translation\", \"\")\n",
    "        elapsed_time = result.get(\"elapsed_time\", 1e-6)  # Default time if not provided\n",
    "        ref_text = data_eng[i] if i < len(data_eng) else \"\"\n",
    "\n",
    "        # Evaluate translation\n",
    "        metric_result = evaluate_translation(\n",
    "            translation, ref_text, target_lang, elapsed_time, all_configs[\"active_metrics\"]\n",
    "        )\n",
    "        for j, metric_name in enumerate(all_configs[\"active_metrics\"]):\n",
    "            metrics[j, i] = metric_result.get(metric_name, np.nan)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_metrics = {metric: np.nanmean(metrics[j, :]) for j, metric in enumerate(all_configs[\"active_metrics\"])}\n",
    "\n",
    "    # Add to leaderboard\n",
    "    new_entry = pd.DataFrame([{\n",
    "        **metadata,\n",
    "        **avg_metrics\n",
    "    }])  # Create a DataFrame for the new entry\n",
    "\n",
    "    # Concatenate the new entry to the leaderboard\n",
    "    leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "    print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "# 리더보드 CSV 파일로 저장\n",
    "leaderboard.to_csv(leaderboard_file, index=False)\n",
    "print(f\"Leaderboard updated and saved to {leaderboard_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and added GPU-A5000_qwen2:72b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Skipping GPU-A5000_llama3.1:70b-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Skipping GPU-A5000_llama3.1-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Leaderboard updated and saved to ./results/translation/English2Korean/leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configurations\n",
    "source_lang = \"English\"  # 소스 언어\n",
    "target_lang = \"Korean\"  # 목표 언어\n",
    "translation_config = f'{source_lang}2{target_lang}'\n",
    "\n",
    "result_dir = f\"./results/translation/{translation_config}/\"\n",
    "leaderboard_file = f\"./results/translation/{translation_config}/leaderboard.csv\"\n",
    "\n",
    "# 기존 리더보드 파일 로드 또는 새로 생성\n",
    "if os.path.exists(leaderboard_file):\n",
    "    leaderboard = pd.read_csv(leaderboard_file)\n",
    "else:\n",
    "    leaderboard = pd.DataFrame(columns=[\n",
    "        \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "        \"BLEU\", \"METEOR\", \"BERTScore\", \"tps\"\n",
    "    ])\n",
    "    \n",
    "# Process each JSON file\n",
    "for filename in os.listdir(result_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    name_head = filename.replace(\".json\", \"\")\n",
    "    metadata = {\n",
    "        \"device-type\": name_head.split(\"-\")[0],\n",
    "        \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "        \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "        \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "        \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "    }\n",
    "\n",
    "    # Skip if already in leaderboard\n",
    "    if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "        (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "        (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "        (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "        (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "        print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "        continue\n",
    "    \n",
    "    # Load translation results\n",
    "    with open(os.path.join(result_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "        translations = json_data.get(\"translations\", [])\n",
    "\n",
    "    # Evaluate translations\n",
    "    num_metrics = 4  # BLEU, METEOR, BERTScore, TPS\n",
    "    metrics = np.full((num_metrics, len(translations)), np.nan)\n",
    "\n",
    "    for i, result in enumerate(translations):\n",
    "        translation = result.get(\"translation\", \"\")\n",
    "        elapsed_time = result.get(\"elapsed_time\", 1e-6)  # Default time if not provided\n",
    "        ref_text = data_kor[i] if i < len(data_kor) else \"\"\n",
    "\n",
    "        # Evaluate translation\n",
    "        metric_result = evaluate_translation(\n",
    "            translation, ref_text, target_lang, elapsed_time, all_configs[\"active_metrics\"]\n",
    "        )\n",
    "        for j, metric_name in enumerate(all_configs[\"active_metrics\"]):\n",
    "            metrics[j, i] = metric_result.get(metric_name, np.nan)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_metrics = {metric: np.nanmean(metrics[j, :]) for j, metric in enumerate(all_configs[\"active_metrics\"])}\n",
    "\n",
    "    # Add to leaderboard\n",
    "    new_entry = pd.DataFrame([{\n",
    "        **metadata,\n",
    "        **avg_metrics\n",
    "    }])  # Create a DataFrame for the new entry\n",
    "\n",
    "    # Concatenate the new entry to the leaderboard\n",
    "    leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "    print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "# 리더보드 CSV 파일로 저장\n",
    "leaderboard.to_csv(leaderboard_file, index=False)\n",
    "print(f\"Leaderboard updated and saved to {leaderboard_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
