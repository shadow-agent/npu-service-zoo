{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서비스 지표 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경 탐지 (GPU, NPU-FuriosaAI RNGD)\n",
    "\n",
    "- 서비스 실행 (예: 번역 + 모니터링)\n",
    "\n",
    "- 스코어 계산하기 \n",
    "\n",
    "- 리더보드에 자동 업데이트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./configuration/config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations Loaded Successfully:\n",
      "Active Metrics: ['BLEU', 'METEOR', 'BERTScore', 'tps']\n",
      "Device Config: {'type': 'GPU', 'model': 'A5000', 'count': 1}\n",
      "Model Config: {'name': 'llama3.3:70b', 'quantization': 'Q4_K_M', 'calibration': 'base'}\n",
      "Evaluation Settings: {'task': 'translation', 'output_dir': './results/translation'}\n",
      "----------------------- \n",
      "\n",
      "Result file does not exist. Proceeding with computation...\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from src.common.load_config import load_all_configurations\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load configuration\n",
    "all_configs = load_all_configurations(config_path)\n",
    "\n",
    "# Set ENABLE_MONITORING=true in the environment to enable monitoring\n",
    "os.environ[\"ENABLE_MONITORING\"] = \"true\" if \"power_consumption\" in all_configs[\"active_metrics\"] else \"false\"\n",
    "    \n",
    "# Print the loaded configurations\n",
    "print(\"Configurations Loaded Successfully:\")\n",
    "print(\"Active Metrics:\", all_configs[\"active_metrics\"])\n",
    "print(\"Device Config:\", all_configs[\"device_config\"])\n",
    "print(\"Model Config:\", all_configs[\"model_config\"])\n",
    "print(\"Evaluation Settings:\", all_configs[\"evaluation_settings\"])\n",
    "\n",
    "# Look up imtermediate result folder to avoid redundancy (performance leaderboard)\n",
    "name_config = (\n",
    "    f'{all_configs[\"device_config\"][\"type\"]}-'\n",
    "    f'{all_configs[\"device_config\"][\"model\"]}_'\n",
    "    f'{all_configs[\"model_config\"][\"name\"]}-'\n",
    "    f'{all_configs[\"model_config\"][\"quantization\"]}_'\n",
    "    f'calib-{all_configs[\"model_config\"].get(\"calibration\", \"none\")}'\n",
    ")\n",
    "result_folder = os.path.join(all_configs[\"evaluation_settings\"][\"output_dir\"], \"Korean2English\")\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.json\")\n",
    "\n",
    "# Ensure result folder exists\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "print(\"----------------------- \\n\")\n",
    "if os.path.exists(result_file):\n",
    "    print(f\"Result file already exists at: {result_file}. Skipping computation.\")\n",
    "    proceed_calc = \"true\"\n",
    "else:\n",
    "    print(f\"Result file does not exist. Proceeding with computation...\")\n",
    "    proceed_calc = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task-specific modules based on the configuration\n",
    "task = all_configs[\"evaluation_settings\"].get(\"task\")\n",
    "\n",
    "if task == \"translation\":\n",
    "    from src.translation.one_translate import (\n",
    "        initialize_translation_environment,\n",
    "        translate_text,\n",
    "        batch_translate_text,\n",
    "    )\n",
    "    from src.metrics.evaluate_translation import evaluate_translation\n",
    "elif task == \"summarization\":\n",
    "    from src.summarization.one_summary import (\n",
    "        initialize_summarization_environment,\n",
    "        summarize_text,\n",
    "        batch_summarize_text,\n",
    "    )\n",
    "    from src.metrics.evaluate_summarization import evaluate_summarization\n",
    "elif task == \"multimodal\":\n",
    "    from src.multimodal.one_multimodal import (\n",
    "        initialize_multimodal_environment,\n",
    "        process_multimodal_data,\n",
    "    )\n",
    "    from src.metrics.evaluate_multimodal import evaluate_multimodal\n",
    "elif task == \"chatbot\":\n",
    "    from src.chatbot.one_chatbot import (\n",
    "        initialize_chatbot_environment,\n",
    "        generate_chat_response,\n",
    "    )\n",
    "    from src.metrics.evaluate_chatbot import evaluate_chatbot\n",
    "else:\n",
    "    raise ValueError(f\"Undetermined task: {task}. Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 벤치마킹 데이터셋 (FLORES-200)\n",
    "\n",
    "- 한 -> 영, 영 -> 한 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "def load_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "# FLORES-200 데이터 위치 \n",
    "data_dir = \"./data/translation/flores\"\n",
    "\n",
    "data_eng = load_text_file(f\"{data_dir}/devtest.eng_Latn\")\n",
    "data_kor = load_text_file(f\"{data_dir}/devtest.kor_Hang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 01:39:18,620 - ERROR - Failed to detect NPUs: [Errno 2] No such file or directory: 'furiosa-smi'\n",
      "2025-01-09 01:39:18,621 - INFO - Translation environment detected: GPU. Using GPU for translations.\n",
      "2025-01-09 01:39:41,918 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:39:45,950 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:39:48,445 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:39:53,725 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:39:57,242 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:00,040 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:01,119 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:04,065 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:05,730 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:07,338 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:09,818 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:12,453 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:15,252 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:18,255 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:21,359 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:25,825 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:29,628 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:31,971 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:35,036 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:40,722 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:42,491 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:44,050 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:47,648 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:51,582 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:53,723 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:55,778 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:40:57,557 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:00,599 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:02,880 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:04,139 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:08,699 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:12,702 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:16,443 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:19,521 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:20,781 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:23,153 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:24,975 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:27,119 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:28,787 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:30,103 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:32,253 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:34,296 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:37,649 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:40,188 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:42,419 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:45,463 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:48,112 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:50,313 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:52,215 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:55,248 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:57,071 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:41:59,413 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:01,150 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:03,283 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:06,890 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:09,986 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:13,486 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:16,622 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:18,488 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:19,757 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:21,486 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:23,233 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:25,856 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:28,559 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:31,441 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:34,095 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:36,349 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:38,500 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:40,964 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:43,711 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:45,532 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:49,302 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:52,260 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:56,181 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:42:59,059 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:02,098 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:05,917 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:07,910 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:08,940 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:11,713 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:13,769 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:15,947 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:18,051 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:21,158 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:22,667 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:24,652 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:27,894 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:30,184 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:32,071 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:34,762 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:37,460 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:39,140 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:42,096 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:43,275 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:45,295 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:47,333 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:49,152 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:51,736 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:54,019 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:43:56,591 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko2en translation done\n",
      "Results saved to ./results/translation/Korean2English/GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 01:44:00,344 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:06,191 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:10,833 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:17,677 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:22,160 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:26,337 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:27,831 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:31,841 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:34,048 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:36,006 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:40,389 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:45,012 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:48,481 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:52,022 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:44:56,520 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:02,041 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:06,672 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:09,910 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:14,773 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:21,810 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:24,223 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:26,619 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:30,516 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:36,444 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:40,027 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:43,001 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:45,897 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:50,410 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:53,699 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:45:55,488 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:01,494 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:07,565 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:11,497 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:15,654 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:17,455 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:20,031 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:24,588 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:27,102 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:29,143 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:30,994 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:34,605 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:36,958 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:40,604 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:43,494 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:46,335 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:50,481 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:54,885 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:46:57,397 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:00,740 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:04,260 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:06,061 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:10,302 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:12,654 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:15,786 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:19,989 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:26,797 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:30,585 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:35,501 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:38,756 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:41,017 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:44,476 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:46,881 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:49,656 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:53,273 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:47:57,520 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:01,057 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:04,752 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:08,292 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:11,047 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:15,753 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:18,424 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:24,055 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:28,163 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:33,265 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:37,369 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:41,419 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:46,206 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:49,334 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:50,724 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:54,858 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:48:58,207 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:00,665 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:03,481 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:07,300 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:09,098 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:11,747 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:16,495 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:20,608 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:23,985 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:27,007 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:30,177 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:33,279 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:37,523 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:39,483 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:43,219 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:45,727 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:48,977 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:52,178 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:54,768 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-01-09 01:49:58,144 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en2ko translation done\n",
      "Results saved to ./results/translation/English2Korean/GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the translation environment\n",
    "try:\n",
    "    initialize_translation_environment(llm_model=all_configs[\"model_config\"][\"name\"])\n",
    "except EnvironmentError as e:\n",
    "    print(f\"Failed to initialize translation environment: {e}\")\n",
    "    exit(1)\n",
    "    \n",
    "# 한 -> 영 번역 \n",
    "num_benchmark = 100\n",
    "start_time = time() # start time\n",
    "batch_results = batch_translate_text(data_kor[:num_benchmark], source_language=\"Korean\", target_language=\"English\")\n",
    "batch_results[\"elapsed_time\"] = time() - start_time  # Add elapsed time to the results\n",
    "result_folder = os.path.join(all_configs[\"evaluation_settings\"][\"output_dir\"], \"Korean2English\")\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.json\")\n",
    "print('ko2en translation done')\n",
    "\n",
    "# Save intermediate result \n",
    "with open(result_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(batch_results, file, indent=4, ensure_ascii=False)  # Save the batch results in JSON format\n",
    "    print(f\"Results saved to {result_file}.\")\n",
    "    \n",
    "# 영 -> 한 번역 \n",
    "start_time = time()\n",
    "batch_results = batch_translate_text(data_eng[:num_benchmark], source_language=\"English\", target_language=\"Korean\")\n",
    "batch_results[\"elapsed_time\"] = time() - start_time  # Add elapsed time to the results\n",
    "result_folder = os.path.join(all_configs[\"evaluation_settings\"][\"output_dir\"], \"English2Korean\")\n",
    "result_file = os.path.join(result_folder, f\"{name_config}.json\")\n",
    "print('en2ko translation done')\n",
    "\n",
    "# Save intermediate result \n",
    "with open(result_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(batch_results, file, indent=4, ensure_ascii=False)  # Save the batch results in JSON format\n",
    "    print(f\"Results saved to {result_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and added GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Skipping GPU-A5000_llama3.1-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Leaderboard updated and saved to ./results/translation/Korean2English/leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "from src.metrics.evaluate_translation import evaluate_translation\n",
    "\n",
    "# Configurations\n",
    "source_lang = \"Korean\"  # 소스 언어\n",
    "target_lang = \"English\"  # 목표 언어\n",
    "translation_config = f'{source_lang}2{target_lang}'\n",
    "\n",
    "result_dir = f\"./results/translation/{translation_config}/\"\n",
    "leaderboard_file = f\"./results/translation/{translation_config}/leaderboard.csv\"\n",
    "\n",
    "# 기존 리더보드 파일 로드 또는 새로 생성\n",
    "if os.path.exists(leaderboard_file):\n",
    "    leaderboard = pd.read_csv(leaderboard_file)\n",
    "else:\n",
    "    leaderboard = pd.DataFrame(columns=[\n",
    "        \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "        \"BLEU\", \"METEOR\", \"BERTScore\", \"tps\"\n",
    "    ])\n",
    "    \n",
    "# Process each JSON file\n",
    "for filename in os.listdir(result_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    name_head = filename.replace(\".json\", \"\")\n",
    "    metadata = {\n",
    "        \"device-type\": name_head.split(\"-\")[0],\n",
    "        \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "        \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "        \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "        \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "    }\n",
    "\n",
    "    # Skip if already in leaderboard\n",
    "    if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "        (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "        (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "        (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "        (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "        print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "        continue\n",
    "    \n",
    "    # Load translation results\n",
    "    with open(os.path.join(result_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "        translations = json_data.get(\"translations\", [])\n",
    "\n",
    "    # Evaluate translations\n",
    "    num_metrics = 4  # BLEU, METEOR, BERTScore, TPS\n",
    "    metrics = np.full((num_metrics, len(translations)), np.nan)\n",
    "\n",
    "    for i, result in enumerate(translations):\n",
    "        translation = result.get(\"translation\", \"\")\n",
    "        elapsed_time = result.get(\"elapsed_time\", 1e-6)  # Default time if not provided\n",
    "        ref_text = data_eng[i] if i < len(data_eng) else \"\"\n",
    "\n",
    "        # Evaluate translation\n",
    "        metric_result = evaluate_translation(\n",
    "            translation, ref_text, target_lang, elapsed_time, all_configs[\"active_metrics\"]\n",
    "        )\n",
    "        for j, metric_name in enumerate(all_configs[\"active_metrics\"]):\n",
    "            metrics[j, i] = metric_result.get(metric_name, np.nan)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_metrics = {metric: np.nanmean(metrics[j, :]) for j, metric in enumerate(all_configs[\"active_metrics\"])}\n",
    "\n",
    "    # Add to leaderboard\n",
    "    new_entry = pd.DataFrame([{\n",
    "        **metadata,\n",
    "        **avg_metrics\n",
    "    }])  # Create a DataFrame for the new entry\n",
    "\n",
    "    # Concatenate the new entry to the leaderboard\n",
    "    leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "    print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "# 리더보드 CSV 파일로 저장\n",
    "leaderboard.to_csv(leaderboard_file, index=False)\n",
    "print(f\"Leaderboard updated and saved to {leaderboard_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/miniconda3/envs/langserve/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and added GPU-A5000_llama3.3:70b-Q4_K_M_calib-base.json to leaderboard.\n",
      "Skipping GPU-A5000_llama3.1-Q4_K_M_calib-base.json, already in leaderboard.\n",
      "Leaderboard updated and saved to ./results/translation/English2Korean/leaderboard.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configurations\n",
    "source_lang = \"English\"  # 소스 언어\n",
    "target_lang = \"Korean\"  # 목표 언어\n",
    "translation_config = f'{source_lang}2{target_lang}'\n",
    "\n",
    "result_dir = f\"./results/translation/{translation_config}/\"\n",
    "leaderboard_file = f\"./results/translation/{translation_config}/leaderboard.csv\"\n",
    "\n",
    "# 기존 리더보드 파일 로드 또는 새로 생성\n",
    "if os.path.exists(leaderboard_file):\n",
    "    leaderboard = pd.read_csv(leaderboard_file)\n",
    "else:\n",
    "    leaderboard = pd.DataFrame(columns=[\n",
    "        \"device-type\", \"device-name\", \"llm\", \"quantization\", \"calibration\",\n",
    "        \"BLEU\", \"METEOR\", \"BERTScore\", \"tps\"\n",
    "    ])\n",
    "    \n",
    "# Process each JSON file\n",
    "for filename in os.listdir(result_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    name_head = filename.replace(\".json\", \"\")\n",
    "    metadata = {\n",
    "        \"device-type\": name_head.split(\"-\")[0],\n",
    "        \"device-name\": name_head.split(\"_\")[0].split(\"-\")[1],\n",
    "        \"llm\": name_head.split(\"_\")[1].split('-')[0],\n",
    "        \"quantization\": name_head.split(\"_calib\")[0].split('-')[-1],\n",
    "        \"calibration\": name_head.split(\"_calib-\")[1],\n",
    "    }\n",
    "\n",
    "    # Skip if already in leaderboard\n",
    "    if ((leaderboard[\"device-type\"] == metadata[\"device-type\"]) &\n",
    "        (leaderboard[\"device-name\"] == metadata[\"device-name\"]) &\n",
    "        (leaderboard[\"llm\"] == metadata[\"llm\"]) &\n",
    "        (leaderboard[\"quantization\"] == metadata[\"quantization\"]) &\n",
    "        (leaderboard[\"calibration\"] == metadata[\"calibration\"])).any():\n",
    "        print(f\"Skipping {filename}, already in leaderboard.\")\n",
    "        continue\n",
    "    \n",
    "    # Load translation results\n",
    "    with open(os.path.join(result_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "        translations = json_data.get(\"translations\", [])\n",
    "\n",
    "    # Evaluate translations\n",
    "    num_metrics = 4  # BLEU, METEOR, BERTScore, TPS\n",
    "    metrics = np.full((num_metrics, len(translations)), np.nan)\n",
    "\n",
    "    for i, result in enumerate(translations):\n",
    "        translation = result.get(\"translation\", \"\")\n",
    "        elapsed_time = result.get(\"elapsed_time\", 1e-6)  # Default time if not provided\n",
    "        ref_text = data_kor[i] if i < len(data_kor) else \"\"\n",
    "\n",
    "        # Evaluate translation\n",
    "        metric_result = evaluate_translation(\n",
    "            translation, ref_text, target_lang, elapsed_time, all_configs[\"active_metrics\"]\n",
    "        )\n",
    "        for j, metric_name in enumerate(all_configs[\"active_metrics\"]):\n",
    "            metrics[j, i] = metric_result.get(metric_name, np.nan)\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_metrics = {metric: np.nanmean(metrics[j, :]) for j, metric in enumerate(all_configs[\"active_metrics\"])}\n",
    "\n",
    "    # Add to leaderboard\n",
    "    new_entry = pd.DataFrame([{\n",
    "        **metadata,\n",
    "        **avg_metrics\n",
    "    }])  # Create a DataFrame for the new entry\n",
    "\n",
    "    # Concatenate the new entry to the leaderboard\n",
    "    leaderboard = pd.concat([leaderboard, new_entry], ignore_index=True)\n",
    "    print(f\"Processed and added {filename} to leaderboard.\")\n",
    "\n",
    "# 리더보드 CSV 파일로 저장\n",
    "leaderboard.to_csv(leaderboard_file, index=False)\n",
    "print(f\"Leaderboard updated and saved to {leaderboard_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
